{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqodCYiIylbf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "# !kaggle datasets download -d ajaysh/amazon-fine-food-reviews --unzip -p /content/data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "dIAwBjPx018N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#using Sqlite3 to connect to the database file\n",
        "conn = sqlite3.connect('/content/data/database.sqlite') # Update to the correct path"
      ],
      "metadata": {
        "id": "iYOAkm5Hyr3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering Data where only positive and negative reviews come in and discarding the reviews with neutral score\n",
        "filtered_data = pd.read_sql_query('''SELECT * FROM Reviews WHERE Score !=3''', conn)\n",
        "\n",
        "#Given reviews with score > 3 a positive rating, and reviews with a score<3 with a negative rating\n",
        "def partition(x):\n",
        "    if x>3:\n",
        "        return 'positive'\n",
        "    return 'negative'\n",
        "\n",
        "#changing review score less than 3 to be positive and vice-versa\n",
        "actialScore = filtered_data['Score']\n",
        "positiveNegative = actialScore.map(partition)\n",
        "filtered_data['Score'] = positiveNegative\n",
        "filtered_data.head()\n"
      ],
      "metadata": {
        "id": "wIQ-a2R-y4sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Data Cleaning: Deduplication\n",
        "\n",
        "display(filtered_data.shape)\n",
        "#Sorting data according to ProductId in ascending order\n",
        "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
        "#Deduplication of entries\n",
        "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
        "display(final.shape)"
      ],
      "metadata": {
        "id": "52txRXOzy9Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking to see how much % of data still remains\n",
        "print(\"The percentage of data remained is: \", (final['Id'].size*1.0) / (filtered_data['Id'].size*1.0)*100)"
      ],
      "metadata": {
        "id": "z-DFibE21Wqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Text Preprocessing: Stemming,stop-word removal and Lemmatization**"
      ],
      "metadata": {
        "id": "I5PCJ6om2JN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download wordnet if not already downloaded\n",
        "nltk.download('wordnet') # Download wordnet to enable lemmatization\n"
      ],
      "metadata": {
        "id": "P9Ek26E_28Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords in English\n",
        "stop  = set(stopwords.words('english'))\n",
        "\n",
        "#Initializing the Snowball stemmer\n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "#Function to clean html elements from text\n",
        "def cleanhtml(sentence):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', sentence)\n",
        "    return cleantext\n",
        "\n",
        "#Function to clean punctuations from text\n",
        "def cleanpunc(sentence):\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]', r' ', sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned)\n",
        "    return cleaned\n"
      ],
      "metadata": {
        "id": "S-ppdJCq1xZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words(BOW)**"
      ],
      "metadata": {
        "id": "tGqvo7dH3LcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = CountVectorizer()\n",
        "final_counts = count_vect.fit_transform(final['Text'].values)\n"
      ],
      "metadata": {
        "id": "XUnoq4a_3RaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find sentences containing HTML Tags\n",
        "\n",
        "i = 0;\n",
        "for sent in final['Text'].values:\n",
        "    if(len(re.findall('<.*?', sent))):\n",
        "        print(i)\n",
        "        print(sent)\n",
        "        break;\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "YG3IIIWT3hiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop)\n",
        "print('******************************************')\n",
        "print(sno.stem('tasty'))"
      ],
      "metadata": {
        "id": "MvuRVvd_3iWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for implementing step-by-step the checks mentioned in the preprocessing\n",
        "i = 0\n",
        "str1 = ' '\n",
        "final_string = []\n",
        "all_positive_words = []\n",
        "all_negative_words = []\n",
        "s=''\n",
        "\n",
        "for sent in final['Text'].values:\n",
        "    filtered_sentence = []\n",
        "    sent = cleanhtml(sent)\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n",
        "                if(cleaned_words.lower() not in stop):\n",
        "                    s = (sno.stem(cleaned_words.lower())).encode('utf-8')\n",
        "                    filtered_sentence.append(s)\n",
        "                    if(final['Score'].values)[i] =='positive':\n",
        "                        all_positive_words.append(s)\n",
        "                    if(final['Score'].values)[i] == 'negative':\n",
        "                        all_negative_words.append(s)\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "    str1 = b\" \".join(filtered_sentence)\n",
        "\n",
        "    final_string.append(str1)\n",
        "    i+=1\n"
      ],
      "metadata": {
        "id": "fxJ4sh5k3uvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final['CleanedText'] = final_string\n",
        "final.head(5)"
      ],
      "metadata": {
        "id": "HQt-_K0336wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('final.sqlite')\n",
        "c = conn.cursor()\n",
        "print(c)\n",
        "conn.text_factory = str\n",
        "final.to_sql('Reviews', conn, schema=None, if_exists='replace')"
      ],
      "metadata": {
        "id": "S_PraTHF4nmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_positive = nltk.FreqDist(all_positive_words)\n",
        "freq_dist_negative = nltk.FreqDist(all_negative_words)\n",
        "print(\"Most Common Positive Words : \", freq_dist_positive.most_common(20))\n",
        "print(\"Most Common Negative Words : \", freq_dist_negative.most_common(20))"
      ],
      "metadata": {
        "id": "2q2ehESC45fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bi-grams, tri-grams, n-grams\n",
        "count_vect = CountVectorizer(ngram_range=(1,2))\n",
        "final_bigram_counts = count_vect.fit_transform(final['Text'].values)\n",
        "print(final_bigram_counts.get_shape())\n"
      ],
      "metadata": {
        "id": "RSWMhpCJ46Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**"
      ],
      "metadata": {
        "id": "8TQMUh1F5Fyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
        "final_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)"
      ],
      "metadata": {
        "id": "1smKkYhR5QWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = tf_idf_vect.get_feature_names_out() # Use get_feature_names_out() instead of get_feature_names()\n",
        "len(features)"
      ],
      "metadata": {
        "id": "e0kMKOcx6PLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[100000:100010]\n"
      ],
      "metadata": {
        "id": "9YEGQI0N6xB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_tf_idf_feats(row, features, top_n = 25):\n",
        "    '''Get top n tfidf values in a row and return them with their corrosponding vector'''\n",
        "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
        "    top_feat = [(features[i], row[i]) for i in topn_ids]\n",
        "    df = pd.DataFrame(top_feat)\n",
        "    df.columns = ['feature', 'tfidf']\n",
        "    return df\n",
        "\n",
        "top_tf_idf = top_tf_idf_feats(final_tf_idf[1,:].toarray()[0], features, 25)\n",
        "\n",
        "print(top_tf_idf)\n"
      ],
      "metadata": {
        "id": "Vc2CwZBw6xsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec**"
      ],
      "metadata": {
        "id": "8lAwpo_67TtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adarshsng/googlenewsvectors --unzip -p /content/data\n"
      ],
      "metadata": {
        "id": "LIF0abIM7IoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Google News Word2Vectors\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "\n",
        "#We are using pretrained model by Google\n",
        "modelw2v = KeyedVectors.load_word2vec_format('/content/data/GoogleNews-vectors-negative300.bin', binary=True, unicode_errors='ignore')"
      ],
      "metadata": {
        "id": "GC5a0bGo82bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelw2v.get_vector('car')\n"
      ],
      "metadata": {
        "id": "G3klyZMA9W-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelw2v.similarity('man', 'woman')"
      ],
      "metadata": {
        "id": "-2yzUtB39rsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelw2v.most_similar('woman')\n"
      ],
      "metadata": {
        "id": "8cnYaiRm9xDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.most_similar('tasti') which is a stem of word tasty\n",
        "#No word found, OOV Issue\n",
        "#Stemmed words may not have word2vecs so be careful in doing it.\n",
        "\n",
        "modelw2v.most_similar('tasty')\n"
      ],
      "metadata": {
        "id": "hMsmkEtV-O-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now Let's create our own Word2Vec model using our own text corpus\n",
        "\n",
        "import gensim\n",
        "\n",
        "i=0\n",
        "list_of_sent = []\n",
        "for sent in final['Text'].values:\n",
        "    filtered_sentence = []\n",
        "    sent = cleanhtml(sent)\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if(cleaned_words.isalpha()):\n",
        "                filtered_sentence.append(cleaned_words.lower())\n",
        "            else:\n",
        "                continue\n",
        "    list_of_sent.append(filtered_sentence)"
      ],
      "metadata": {
        "id": "rVCkQZ2B-QCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final['Text'].values[0])\n",
        "print(\"***********************************************************************\")\n",
        "print(list_of_sent[0])"
      ],
      "metadata": {
        "id": "lmnMtwNB-QcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = gensim.models.Word2Vec(list_of_sent, min_count=5, vector_size=50, workers=4)\n"
      ],
      "metadata": {
        "id": "Nj6HiKIo-Q-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(w2v_model.wv.index_to_key)\n",
        "print(len(words))\n"
      ],
      "metadata": {
        "id": "JwVSf1Pi-bd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar('tasty')\n",
        "#Now we have more similar words from our own model"
      ],
      "metadata": {
        "id": "KKultYYN-hEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar('like')\n",
        "#Now we have more similar words from our own model"
      ],
      "metadata": {
        "id": "4DUswlAY-hA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect_feat = count_vect.get_feature_names_out() # Use get_feature_names_out() instead of get_feature_names()\n",
        "# Convert count_vect_feat to a list to use the index() method\n",
        "count_vect_feat = list(count_vect_feat)\n",
        "index_of_like = count_vect_feat.index('like')\n",
        "print(count_vect_feat[64055])"
      ],
      "metadata": {
        "id": "cnIQgyGD-g9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average Word2Vec\n",
        "#Compute average word2vec for each review\n",
        "\n",
        "sent_vectors = [];\n",
        "for sent in list_of_sent:\n",
        "    sent_vec = np.zeros(50)\n",
        "    cnt_words = 0\n",
        "    for word in sent:\n",
        "        try:\n",
        "            vec = w2v_model[word]\n",
        "            sent_vec+=vec\n",
        "            cnt_words +=1\n",
        "        except:\n",
        "            pass\n",
        "    sent_vec /=cnt_words\n",
        "    sent_vectors.append(sent_vec)\n",
        "print(len(sent_vectors))\n",
        "print(len(sent_vectors[0]))"
      ],
      "metadata": {
        "id": "h9WIu1ab-gy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF weighted Word2Vec\n",
        "tfidf_feat = tf_idf_vect.get_feature_names_out()\n",
        "\n",
        "tfidf_sent_vectors = []\n",
        "row = 0;\n",
        "\n",
        "for sent in list_of_sent:\n",
        "    sent_vec = np.zeros(50)\n",
        "    weight_sum = 0\n",
        "    for word in sent:\n",
        "        try:\n",
        "            vec = w2v_model[word]\n",
        "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
        "            sent_vec+=(vec*tf_idf)\n",
        "            weight_sum+=tf_idf\n",
        "        except:\n",
        "            pass\n",
        "    sent_vec /=weight_sum\n",
        "    tfidf_sent_vectors.append(sent_vec)\n",
        "    row+=1\n"
      ],
      "metadata": {
        "id": "tqp5wqNI-gSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnUJcDOx-f6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}